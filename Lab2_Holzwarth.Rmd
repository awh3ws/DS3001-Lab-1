---
title: 'Programming Lab #2'
author: "Machine Learning 1"
output: pdf_document
---
# Programming Lab 2

  The purpose of this project is to build predictive algorithms that predict the likelihood a person has a stroke. The data include:
  
  - `age`: Patient age, numeric
  - `avg_glucose_level`: Blood sugar levels, numeric
  - `bmi`: Body mass index, numeric
  - `ever_married`: Ever married, dummy/character (Yes, No)
  - `gender`: Sex, dummy/character
  - `heart_disease`: Has heart disease, dummy
  - `hypertension`: Has hypertension, dummy
  - `id`: Study identification number
  - `Residence_type`: Type of residence, dummy/character (Urban, Rural)
  - `smoking_status`: Former, never, or current smoker, categorical
  - `work_type`: Employment type (Never worked (Never_worked), homemaker ("children"), Public sector employment (Govt_job), Private sector employment (`Private`), Self-employed (`Self-employed`)
  - `stroke`: Suffered a stroke in the sample period
  
  The data come in two files: `training_data.csv`, which you should use to build your models, and `testing_data.csv`, which you should use to test your models. The models must be trained on the training data and tested on the testing data, but providing both files allows you to experiment with your choices and iterate on model designs. If performance drops on the testing data, you know there's a problem.
  
  You can use any of the tools presented in class: $k$ nearest neighbor, linear models, or decision trees. In principle, $k$ means clustering might also be helpful for looking for patterns in the data that the other methods might miss. Using canned versions of more advanced tools (boosting, bagging, random forests, neural networks, etc.) is deeply unsporting and thus not allowed. You can be creative about transforming variables, or combining decision trees with linear models or $k$NN. Try something interesting. Fail extravagantly. The goal is to work on an intellectually interesting question that is similar to the tasks that data scientists are called on to do every day.
  
  We will compare the groups' models to see if there are common trends or significant differences, and also to declare **The Winners** on the basis of whichever team achieves the highest $R^2$ on the testing data. A very lazy `lm(stroke~.,data=testing_data)` regression achieves an in-sample $R^2$ of $0.1569$, so there appears to be evidence of meaningful connection between the variables and outcome and there should be opportunity to improve. I am mostly interested in the choices you make about cleaning the data, transforming variables, and building models, not the $R^2$, but it is a useful summary statistic to consider and motivate thoughtful choices.
  
  This is supposed to be fairly "fun," so please do not turn it into a combinatorial nightmare of comparing thousands of model specifications. Settle on a strategy you think is promising, crank it out, and write up the results. Your time and energy are valuable, so learn to recognize when the marginal cost of another twenty minutes on a project exceeds the benefit in terms of improving the results and your grade.
  
## Paper format

The format of the paper should be:

  - Summary: A one paragraph description of the question, methods, and results (about 350 words).
  - Data: One to two pages discussing the data and key variables, and any challenges in reading, cleaning, and preparing them for analysis.
  - Results: Two to five pages providing visualizations, statistics, a discussion of your methodology, and a presentation of your main findings. 
  - Conclusion: One to two pages summarizing the project, defending it from criticism, and suggesting additional work that was outside the scope of the project.
  - Appendix: If you have a significant number of additional plots or table that you feel are essential to the project, you can put any amount of extra content at the end and reference it from the body of the paper. 

## Submission

Each student should upload a zip folder to the Assignments tab on Canvas, which includes

  - Your cleaned data set
  - .R or .Rmd files that clean the data and conduct the analysis
  - The paper, written in .Rmd format and compiled to a .html or .pdf file

Each student can submit their own work, despite being in a group, or the group can collaborate on a single submission that all members submit.

## Criteria

The project is graded based on four criteria:

  - Project Concept: What is the strategy for building and testing the group's models? How did the group decide how to use the tools presented so far in class? How did the group compare the performance of the options considered, and settle on a final choice for submission?
  - Wrangling, EDA, and Visualization: How are are missing values handled? For variables with large numbers of missing values, to what extent do the data and documentation provide an explanation for the missing data? If multiple data sources are used, how are the data merged? For the main variables in the analysis, are the relevant data summarized and visualized through a histogram or kernel density plot where appropriate? Are basic quantitative features of the data addressed and explained? How are outliers characterized and addressed? 
  - Analysis: What are the groups' main findings? Do the tables, plots, and statistics support the conclusions? Is the research strategy carried out correctly? If the research strategy succeeds, are the results interpreted correctly and appropriately? If the research strategy fails, is a useful discussion of the flaws of the data collection process or the research strategy discussed?
  - Replication/Documentation: Is the code appropriately commented? Can the main results be replicated from the code and original data files? Are significant choices noted and explained?

Each of the four criteria are equally weighted (25 points out of 100).


```{r}
#import data
df_train <- read.csv("training_data.csv")
df_test <- read.csv("testing_data.csv")

#data wrangle
df_train <- df_train[-c(1,2)]
df_test <- df_test[-c(1,2)]

df_train$bmi = as.numeric(df_train$bmi)
df_test$bmi = as.numeric(df_test$bmi)

df_train$gender <- ifelse(df_train$gender == "Male",1,0)
df_train$ever_married <- ifelse(df_train$ever_married == "Yes",1,0)
df_train$Residence_type <- ifelse(df_train$Residence_type == "Urban",1,0)

df_test$gender <- ifelse(df_test$gender == "Male",1,0)
df_test$ever_married <- ifelse(df_test$ever_married == "Yes",1,0)
df_test$Residence_type <- ifelse(df_test$Residence_type == "Urban",1,0)

df_train <- na.omit(df_train)
df_test <- na.omit(df_test)


summary(df_train)
N = dim(df_train)[1] + dim(df_test)[1]
```

```{r}
#Linear Model 1
lazy_lm <- lm(stroke~.,data=df_train)
summary(lazy_lm)

y_hat_linear1 = predict.lm(lazy_lm,df_test)

y_bar1 = mean(df_train$stroke) # Average value of training data outcomes
y_test1 = df_test$stroke # True y-values vector
TSS1 = sum( (y_test1 - y_bar1)^2 ) # Compute total sum of squares

r_sq_linearModel1 = 1 - sum( (y_test1 - y_hat_linear1)^2 )/TSS1
rmse_linearModel1 = sqrt((1/N)*sum((y_test1 - y_hat_linear1)^2))

print(r_sq_linearModel1)
print(rmse_linearModel1)
```
```{r}
#Linear Model 2
model_2 <- lm(stroke~age+hypertension+heart_disease+ever_married+avg_glucose_level+work_type,data=df_train)
summary(model_2)

y_hat_linear2 = predict.lm(model_2,df_test)

y_bar2 = mean(df_train$stroke) # Average value of training data outcomes
y_test2 = df_test$stroke # True y-values vector
TSS2 = sum( (y_test2 - y_bar2)^2 ) # Compute total sum of squares

r_sq_linearModel2 = 1 - sum( (y_test2 - y_hat_linear2)^2 )/TSS2
rmse_linearModel2 = sqrt((1/N)*sum((y_test2 - y_hat_linear2)^2))

print(r_sq_linearModel2)
print(rmse_linearModel2)
```

```{r}
#Linear Model 3
model_3 <- lm(stroke~age+hypertension+heart_disease,data=df_train)
summary(model_3)

y_hat_linear3 = predict.lm(model_3,df_test)

y_bar3 = mean(df_train$stroke) # Average value of training data outcomes
y_test3 = df_test$stroke # True y-values vector
TSS3 = sum( (y_test3 - y_bar3)^2 ) # Compute total sum of squares

r_sq_linearModel3 = 1 - sum( (y_test3 - y_hat_linear3)^2 )/TSS3
rmse_linearModel3 = sqrt((1/N)*sum((y_test3 - y_hat_linear3)^2))

print(r_sq_linearModel3)
print(rmse_linearModel3)
```

```{r}
#Linear Model 4
model_4 <- lm(stroke~age+hypertension,data=df_train)
summary(model_4)

y_hat_linear4 = predict.lm(model_4,df_test)

y_bar4 = mean(df_train$stroke) # Average value of training data outcomes
y_test4 = df_test$stroke # True y-values vector
TSS4 = sum( (y_test4 - y_bar4)^2 ) # Compute total sum of squares

r_sq_linearModel4 = 1 - sum( (y_test4 - y_hat_linear4)^2 )/TSS4
rmse_linearModel4 = sqrt((1/N)*sum((y_test4 - y_hat_linear4)^2))

print(r_sq_linearModel4)
print(rmse_linearModel4)
```

```{r}
#Linear Model 5
model_5 <- lm(stroke~hypertension+heart_disease,data=df_train)
summary(model_5)

y_hat_linear5 = predict.lm(model_5,df_test)

y_bar5 = mean(df_train$stroke) # Average value of training data outcomes
y_test5 = df_test$stroke # True y-values vector
TSS5 = sum( (y_test5 - y_bar5)^2 ) # Compute total sum of squares

r_sq_linearModel5 = 1 - sum( (y_test5 - y_hat_linear5)^2 )/TSS5
rmse_linearModel5 = sqrt((1/N)*sum((y_test5 - y_hat_linear5)^2))

print(r_sq_linearModel5)
print(rmse_linearModel5)
```

```{r}
#Linear Model 6
model_6 <- lm(stroke~.+hypertension*heart_disease+hypertension*age+heart_disease*age,data=df_train)
summary(model_6)

y_hat_linear6 = predict.lm(model_6,df_test)

y_bar6 = mean(df_train$stroke) # Average value of training data outcomes
y_test6 = df_test$stroke # True y-values vector
TSS6 = sum( (y_test6 - y_bar6)^2 ) # Compute total sum of squares

r_sq_linearModel6 = 1 - sum( (y_test6 - y_hat_linear6)^2 )/TSS6
rmse_linearModel6 = sqrt((1/N)*sum((y_test6 - y_hat_linear6)^2))

print(r_sq_linearModel6)
print(rmse_linearModel6)
```